{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "\n",
    "    pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import xmltodict\n",
    "import pyjq\n",
    "import boto3\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from eu_funding.visualization.visualize import pdf_cdf\n",
    "from eu_funding.utils.misc_utils import print_nested_structure\n",
    "from eu_funding.data.s3_transfer import get_files_from_s3\n",
    "from eu_funding.data.openaire import parse_openaire_records, parse_publications_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'im-eurito'\n",
    "FOLDER = 'external/openaire/projectssoups'\n",
    "KEY_PREFIX = 'soup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for file in get_files_from_s3(bucket=BUCKET, folder=FOLDER, key_prefix=KEY_PREFIX):\n",
    "    records.extend(parse_openaire_records(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(inter_data_path, 'openaire_projects.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'im-eurito'\n",
    "FOLDER = 'external/openaire/publicationssoups'\n",
    "KEY_PREFIX = 'soup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_publications():\n",
    "    records = []\n",
    "    for file in os.listdir(os.path.join(openaire_publication_data_path)):\n",
    "        file_number = file.split('.')[0].split('_')[-1]\n",
    "        if '.txt' in file:\n",
    "            with open(os.path.join(openaire_publication_data_path, file), mode='rb') as f:\n",
    "                data = f.read()\n",
    "                soup = BeautifulSoup(data)\n",
    "                rec = parse_publications_soup(soup)\n",
    "                records.extend(rec)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_publications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i, record in enumerate(chunks(records, 1000)):\n",
    "    i += 1\n",
    "    df = pd.DataFrame().from_records(record)\n",
    "    df.to_csv(\n",
    "        os.path.join(openaire_publication_data_path, 'csv', 'publications_parsed_{:03}.csv'.format(i)),\n",
    "        index=False\n",
    "    )\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Missing PubMed DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from eu_funding.utils.misc_utils import chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_converter(pub_ids, id_type):\n",
    "    id_converter_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/'\n",
    "    params = {\n",
    "        'idtype': id_type,\n",
    "        'ids': ', '.join([str(i) for i in pub_ids]),\n",
    "        'email': 'george.richardson@nesta.org.uk',\n",
    "        'tool': 'eu_funding_analytics'\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url=id_converter_url,\n",
    "        params=params,\n",
    "    )\n",
    "    results = response.content\n",
    "    return results\n",
    "    \n",
    "def parse_id_converter_result(results, id_type):\n",
    "    soup = BeautifulSoup(results)\n",
    "    records = [record.attrs for record in soup.findAll('record')]\n",
    "    for r in records:\n",
    "        r['pid_type'] = id_type\n",
    "    return records\n",
    "\n",
    "def convert_ids(pub_ids, id_type):\n",
    "    pub_id_chunks = chunks(pub_ids, 200)\n",
    "    converted = []\n",
    "    for chunk in pub_id_chunks:\n",
    "        results = get_id_converter(chunk, id_type)\n",
    "        records = parse_id_converter_result(results, id_type)\n",
    "        converted.extend(records)\n",
    "        sleep(3)\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_type = 'pmid'\n",
    "pub_ids = publications_df[publications_df['pid_type'] == id_type]['pid'].values\n",
    "\n",
    "pmid_converted_ids = convert_ids(pub_ids, id_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doi_col(pid, pid_type):\n",
    "    if pid_type == 'doi':\n",
    "        return pid\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "publications_df['doi'] = publications_df.apply(lambda x: doi_col(x['pid'], x['pid_type']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_df = pd.DataFrame().from_records(pmid_converted_ids)\n",
    "pmid_doi_map = {k: v for k, v in zip(\n",
    "    pmid_df['pmid'], pmid_df['doi']\n",
    ")}\n",
    "publications_df['doi'][publications_df['pid_type'] == 'pmid'] = publications_df['pid'].map(pmid_doi_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_type = 'pmc'\n",
    "pub_ids = publications_df[publications_df['pid_type'] == id_type]['pid'].values\n",
    "\n",
    "pmcid_converted_ids = convert_ids(pub_ids, 'pmcid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid_df = pd.DataFrame().from_records(pmcid_converted_ids)\n",
    "pmcid_doi_map = {k[3:]: v for k, v in zip(\n",
    "    pmcid_df['pmcid'], pmcid_df['doi']\n",
    ")}\n",
    "publications_df['doi'][publications_df['pid_type'] == 'pmc'] = publications_df['pid'].map(pmcid_doi_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.to_csv(os.path.join(inter_data_path, 'openaire_publications.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df = pd.read_csv(os.path.join(inter_data_path, 'openaire_publications.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doi_crossref(title, max_rows=5):\n",
    "    title = title.lower()\n",
    "    r = requests.get(\n",
    "    'https://api.crossref.org/works?rows=5&query.title={}'.format(title)\n",
    "    )\n",
    "    doi = np.nan\n",
    "    if r.status_code == 200:\n",
    "        j = r.json()\n",
    "        results = j['message']['items']\n",
    "        dist_max = 0\n",
    "\n",
    "        for result in results:\n",
    "            result_title = result['title'][0].lower()\n",
    "            dist = fuzz.ratio(title, result_title)\n",
    "            if dist < 90:\n",
    "                continue\n",
    "            elif dist == 100:\n",
    "                doi = result['DOI']\n",
    "            elif 100 > dist >= 90:\n",
    "                if dist > dist_max:\n",
    "                    doi = result['DOI']\n",
    "                    dist_max = dist\n",
    "    return doi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Etiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eu_funding.utils.misc_utils import chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = publications_df['title'][pd.isnull(publications_df['doi'])].str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = 20\n",
    "timeout = 30\n",
    "\n",
    "for i, titles in enumerate(chunks(all_titles, 1000)):\n",
    "    out = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=connections) as executor:\n",
    "        future_to_url = (executor.submit(get_doi_crossref, title.decode(), timeout) for title in titles)\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                data = str(type(exc))\n",
    "            finally:\n",
    "                out.append(data)\n",
    "                \n",
    "    with open(os.path.join(inter_data_path, 'openaire_missing_dois', 'dois_{:03}.txt'.format(i)), 'w') as f:\n",
    "        for o in out:\n",
    "            f.write(str(o) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dois = []\n",
    "files = os.listdir(os.path.join(inter_data_path, 'openaire_missing_dois'))\n",
    "for file in files:\n",
    "    with open(os.path.join(inter_data_path, 'openaire_missing_dois', file), 'r') as f:\n",
    "        missing_dois.extend(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df['doi'].loc[all_titles.index] = missing_dois\n",
    "publications_df['doi'][publications_df['doi'] == 'nan'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.to_csv(os.path.join(inter_data_path, 'openaire_publications_20190702.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossRef Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Etiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquette = Etiquette(\n",
    "    application_version='0.1',\n",
    "    application_url='http://www.eurito.eu/',\n",
    "    application_name='eu_funding_analytics',\n",
    "    contact_email='george.richardson@nesta.org.uk',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crossref_work(doi):\n",
    "    works = Works(etiquette=etiquette)\n",
    "    response = works.doi(doi)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dois = publications_df['doi'][~pd.isnull(publications_df['doi'])].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dois = all_dois[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_chunks = list(chunks(all_dois, 1000))\n",
    "doi_chunk_indices = list(range(len(doi_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "connections = 2 # API will rate limit occasionally with just 2 connections so needs babysitting\n",
    "\n",
    "for i, dois in zip(doi_chunk_indices[start:], doi_chunks[start:]):\n",
    "    out = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=connections) as executor:\n",
    "        future_to_url = (executor.submit(get_crossref_work, doi) for doi in dois)\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            data = future.result()\n",
    "            out.append(data)\n",
    "                \n",
    "    with open(os.path.join(ext_data_path, 'crossref', 'works_{:04}.txt'.format(i)), 'w') as f:\n",
    "        json.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
