{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GtR Topic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "from eu_funding.visualization.visualize import pdf_cdf\n",
    "# from src.visualization.visualize import pdf_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesta.packages.nlp_utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['research_topics', 'research_subjects']\n",
    "\n",
    "gtr_projects_df = pd.read_csv(\n",
    "    os.path.join(ext_data_path, 'gtr', 'gtr_projects.csv'),\n",
    "    converters={k: ast.literal_eval for k in list_cols}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_subject_counter = Counter(chain(*gtr_projects_df['research_subjects']))\n",
    "research_topic_counter = Counter(chain(*gtr_projects_df['research_topics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique research subjects in the GtR projects dataset.'.format(len(research_subject_counter)))\n",
    "print('There are {} unique research topics in the GtR projects dataset.'.format(len(research_topic_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_subject_counter.most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Definition Through Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = list(chain(*[sorted(itertools.combinations(d, 2)) for d in gtr_projects_df['research_topics']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic_edge_counter = Counter(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_research_topics = len(list(chain(*gtr_projects_df['research_topics'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(combo, occurrences, cooccurrences, total):\n",
    "    return (2 * total * cooccurrences[combo]) / (occurrences[combo[0]] * occurrences[combo[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = set(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_strengths = [association_strength(\n",
    "    edge,\n",
    "    research_topic_counter, \n",
    "    research_topic_edge_counter, \n",
    "    total_research_topics) for edge in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(assoc_strengths), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.DataFrame()\n",
    "edge_df['source'] = [e[0] for e in edges]\n",
    "edge_df['target'] = [e[1] for e in edges]\n",
    "edge_df['weight'] = np.log10(assoc_strengths)\n",
    "g = nx.from_pandas_edgelist(edge_df, edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityPartition:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "    \n",
    "    def edgelist_to_cooccurrence(self, repeats, **best_partition_kwargs):\n",
    "        edge_counter = Counter()\n",
    "        for i in range(repeats):\n",
    "            partition = community.best_partition(self.graph, **best_partition_kwargs)\n",
    "            edgelist = self.partition_to_edgelist(partition)\n",
    "            edge_counter.update(edgelist)\n",
    "\n",
    "        g = nx.Graph()\n",
    "        g.add_weighted_edges_from([(e[0][0], e[0][1], e[1]) for e in edge_counter.items()])\n",
    "        return g\n",
    "    \n",
    "    def partition_to_edgelist(self, partition):\n",
    "        partition_reverse_mapping = self.reverse_index_partition(partition)\n",
    "        edgelist = []\n",
    "        for community, elements in partition_reverse_mapping.items():\n",
    "            combos = [tuple(sorted(e)) for e in itertools.combinations(elements, 2)]\n",
    "            edgelist.extend(combos)\n",
    "        return edgelist\n",
    "     \n",
    "    def reverse_index_partition(self, partition):\n",
    "        partition_reverse_mapping = defaultdict(list)\n",
    "        for k, v in partition.items():\n",
    "            partition_reverse_mapping[v].append(k)\n",
    "        return partition_reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CommunityPartition(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cp.edgelist_to_cooccurrence(3, resolution=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the best partition\n",
    "part = community.best_partition(co, resolution=0.3, random_state=0, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(part.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = float(len(set(part.values())))\n",
    "pos = nx.spring_layout(co)\n",
    "count = 0.\n",
    "for com in set(part.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in part.keys()\n",
    "                                if part[nodes] == com]\n",
    "    nx.draw_networkx_nodes(co, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(co, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(part).reset_index(drop=False).groupby(0)['index'].apply(lambda x: print(', '.join(list(x))+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name_lookup = {\n",
    "    0: 'social_science',\n",
    "    1: 'social_science',\n",
    "    2: 'arts_humanities',\n",
    "    3: 'social_science',\n",
    "    4: 'arts_humanities',\n",
    "    5: 'biological',\n",
    "    6: 'engineering',\n",
    "    7: 'engineering',\n",
    "    8: 'maths_computing',\n",
    "    9: 'physical_sciences',\n",
    "    10: 'arts_humanities',\n",
    "    11: 'social_science',\n",
    "    12: 'physical_sciences',\n",
    "}\n",
    "\n",
    "topic_discipline_lookup = {top:category_name_lookup[disc] for top,disc in part.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['discipline'] = gtr_projects_df['research_topics'].apply(\n",
    "    lambda x: [topic_discipline_lookup[val] for val in x])\n",
    "\n",
    "gtr_projects_df['discipline_sets'] = [set(x) for x in gtr_projects_df['discipline']]\n",
    "\n",
    "gtr_projects_df['single_disc'] = [True if len(x)==1 else np.nan if len(x)==0 else False for x in gtr_projects_df['discipline_sets']]\n",
    "\n",
    "gtr_projects_df['single_disc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['discipline_sets'] = [\n",
    "    set(['medical_sciences']) if f =='MRC' else x for f,x in zip(\n",
    "        gtr_projects_df['funder_name'],\n",
    "           gtr_projects_df['discipline_sets'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modal_value(l):\n",
    "    c = Counter(l)\n",
    "    try:\n",
    "        return c.most_common(1)[0][0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "gtr_projects_df['modal_discipline'] = [modal_value(d) for d in gtr_projects_df['discipline_sets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['modal_discipline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(chain(*gtr_projects_df['discipline_sets'])).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = [True if len(s) > 0 else False for s in gtr_projects_df['discipline_sets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove projects without abstracts\n",
    "gtr_projects_df = gtr_projects_df[~pd.isnull(gtr_projects_df['abstract_texts'])]\n",
    "# remove projects with short abstracts\n",
    "gtr_projects_df = gtr_projects_df[gtr_projects_df['abstract_texts'].str.len() > 250]\n",
    "# remove projects with no labels\n",
    "n_labels = [True if len(s) > 0 else False for s in gtr_projects_df['discipline_sets']]\n",
    "gtr_projects_df = gtr_projects_df[n_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(raw_data_path, 'stopwords_en_long.txt'), 'r') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markup(text):\n",
    "    tags = ['<b>', '<p>', '&nbsp;', '<li>', '<ol>', '<ul>', '<br>',\n",
    "           '</b>', '</p>', '&nbsp;', '</li>', '</ol>', '</ul>', '</br>',\n",
    "           '\\n', '\\t', '\\r']\n",
    "    for tag in tags:\n",
    "        text = text.replace(tag, ' ')\n",
    "    text = re.sub(\"\\d+\", \"XXX\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [remove_markup(a) for a in gtr_projects_df['abstract_texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stopword in stopwords:\n",
    "    nlp.vocab[stopword.lower()].is_stop = True\n",
    "    nlp.vocab[stopword.upper()].is_stop = True\n",
    "    nlp.vocab[stopword.title()].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_docs = [nlp(d) for d in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    tokenized_doc = []\n",
    "    for t in doc:\n",
    "        if len(t) < 3:\n",
    "            continue\n",
    "        if t.is_stop:\n",
    "            continue\n",
    "        if t.like_num:\n",
    "            continue\n",
    "        if t.is_digit:\n",
    "            continue\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        if t.like_url:\n",
    "            continue\n",
    "        pos = t.pos_.upper()\n",
    "        token = t.lemma_\n",
    "        tokenized_doc.append(f'{token}{pos}')\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_tokenized = [tokenize(doc) for doc in abstract_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Phrases(abstracts_tokenized, delimiter=b'x')\n",
    "bigrammer = Phraser(bigrams)\n",
    "abstracts_bigrammed = bigrammer[abstracts_tokenized]\n",
    "# dictionary = Dictionary(abstracts_bigrammed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_str = [' '.join(d) for d in abstracts_bigrammed]\n",
    "tfidf = TfidfVectorizer(\n",
    "#     max_df=0.5, \n",
    "    min_df=5, \n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "tfidf_vecs = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(chain(*gtr_projects_df['discipline_sets'])))\n",
    "mlb = MultiLabelBinarizer(classes=classes)\n",
    "target_binarized = mlb.fit_transform(gtr_projects_df['discipline_sets'])\n",
    "target_binarized_df = pd.DataFrame(target_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, target_binarized_df, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = []\n",
    "indices = np.array(range(0, X_train.shape[1]))\n",
    "for discipline in y_train.columns:\n",
    "    features_chi2 = chi2(X_train, y_train[discipline])[0]\n",
    "    threshold = np.percentile(features_chi2[~pd.isnull(features_chi2)], 90)\n",
    "    discipline_indices = indices[features_chi2 > threshold]\n",
    "    feature_terms.extend(np.array(tfidf.get_feature_names())[discipline_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stop_words = set(tfidf.get_feature_names()).difference(set(feature_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "#     max_df=0.5, \n",
    "    min_df=5, \n",
    "    sublinear_tf=True, \n",
    "    norm='l2',\n",
    "    stop_words=tfidf_stop_words\n",
    ")\n",
    "tfidf_vecs_filt = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs_filt, target_binarized, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mnb = make_pipeline_imb(\n",
    "                 MultinomialNB()\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=3)\n",
    "\n",
    "pipe_lr = make_pipeline_imb(\n",
    "    LogisticRegression(n_jobs=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[1]):\n",
    "    print(mlb.classes_[i])\n",
    "    clf.fit(X_train, y_train[:, i])\n",
    "    print(classification_report(y_test[:, i], clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
