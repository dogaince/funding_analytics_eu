{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GtR Topic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from eu_funding.visualization.visualize import pdf_cdf\n",
    "from eu_funding.utils.nlp_utils import remove_markup, normalise_digits, lemmatize, bigram, stringify_docs\n",
    "# from src.visualization.visualize import pdf_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nesta.packages.nlp_utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['research_topics', 'research_subjects']\n",
    "\n",
    "gtr_projects_df = pd.read_csv(\n",
    "    os.path.join(ext_data_path, 'gtr', 'gtr_projects.csv'),\n",
    "    converters={k: ast.literal_eval for k in list_cols}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_subject_counter = Counter(chain(*gtr_projects_df['research_subjects']))\n",
    "research_topic_counter = Counter(chain(*gtr_projects_df['research_topics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique research subjects in the GtR projects dataset.'.format(len(research_subject_counter)))\n",
    "print('There are {} unique research topics in the GtR projects dataset.'.format(len(research_topic_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_subject_counter.most_common(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Definition Through Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = list(chain(*[sorted(itertools.combinations(d, 2)) for d in gtr_projects_df['research_topics']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic_edge_counter = Counter(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_research_topics = len(list(chain(*gtr_projects_df['research_topics'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(combo, occurrences, cooccurrences, total):\n",
    "    return (2 * total * cooccurrences[combo]) / (occurrences[combo[0]] * occurrences[combo[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = set(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_strengths = [association_strength(\n",
    "    edge,\n",
    "    research_topic_counter, \n",
    "    research_topic_edge_counter, \n",
    "    total_research_topics) for edge in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(assoc_strengths), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.DataFrame()\n",
    "edge_df['source'] = [e[0] for e in edges]\n",
    "edge_df['target'] = [e[1] for e in edges]\n",
    "edge_df['weight'] = np.log10(assoc_strengths)\n",
    "g = nx.from_pandas_edgelist(edge_df, edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunityPartition:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "    \n",
    "    def edgelist_to_cooccurrence(self, repeats, **best_partition_kwargs):\n",
    "        edge_counter = Counter()\n",
    "        for i in range(repeats):\n",
    "            partition = community.best_partition(self.graph, **best_partition_kwargs)\n",
    "            edgelist = self.partition_to_edgelist(partition)\n",
    "            edge_counter.update(edgelist)\n",
    "\n",
    "        g = nx.Graph()\n",
    "        g.add_weighted_edges_from([(e[0][0], e[0][1], e[1]) for e in edge_counter.items()])\n",
    "        return g\n",
    "    \n",
    "    def partition_to_edgelist(self, partition):\n",
    "        partition_reverse_mapping = self.reverse_index_partition(partition)\n",
    "        edgelist = []\n",
    "        for community, elements in partition_reverse_mapping.items():\n",
    "            combos = [tuple(sorted(e)) for e in itertools.combinations(elements, 2)]\n",
    "            edgelist.extend(combos)\n",
    "        return edgelist\n",
    "     \n",
    "    def reverse_index_partition(self, partition):\n",
    "        partition_reverse_mapping = defaultdict(list)\n",
    "        for k, v in partition.items():\n",
    "            partition_reverse_mapping[v].append(k)\n",
    "        return partition_reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CommunityPartition(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cp.edgelist_to_cooccurrence(3, resolution=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the best partition\n",
    "part = community.best_partition(g, resolution=0.4, random_state=0, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(part.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = float(len(set(part.values())))\n",
    "pos = nx.spring_layout(co)\n",
    "count = 0.\n",
    "for com in set(part.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in part.keys()\n",
    "                                if part[nodes] == com]\n",
    "    nx.draw_networkx_nodes(co, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(co, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(part).reset_index(drop=False).groupby(0)['index'].apply(lambda x: print(', '.join(list(x))+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name_lookup = {\n",
    "    0: 'social_sciences',\n",
    "    1: 'arts_linguistics',\n",
    "    2: 'chem_mater_phys_eng',\n",
    "    3: 'social_sciences',\n",
    "    4: 'maths_computing_ee',\n",
    "    5: 'social_sciences',\n",
    "    6: 'social_sciences',\n",
    "    7: 'biological_sciences',\n",
    "    8: 'social_sciences',\n",
    "    9: 'humanities',\n",
    "    10: 'arts_linguistics',\n",
    "    11: 'humanities',\n",
    "    12: 'social_sciences',\n",
    "    13: 'physics',\n",
    "    14: 'environmental_sciences',\n",
    "    15: 'social_sciences',\n",
    "    16: 'humanities'\n",
    "}\n",
    "\n",
    "topic_discipline_lookup = {top:category_name_lookup[disc] for top, disc in part.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path, 'communities_partition.pkl'), 'wb') as f:\n",
    "    pickle.dump(part, f)\n",
    "\n",
    "with open(os.path.join(model_path, 'communities_partition_labels.pkl'), 'wb') as f:\n",
    "    pickle.dump(category_name_lookup, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['discipline'] = gtr_projects_df['research_topics'].apply(\n",
    "    lambda x: [topic_discipline_lookup[val] for val in x])\n",
    "\n",
    "gtr_projects_df['discipline_sets'] = [set(x) for x in gtr_projects_df['discipline']]\n",
    "\n",
    "gtr_projects_df['single_disc'] = [True if len(x)==1 else np.nan if len(x)==0 else False for x in gtr_projects_df['discipline_sets']]\n",
    "\n",
    "gtr_projects_df['single_disc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['discipline_sets'] = [\n",
    "    set(['medical_sciences']) if f =='MRC' else x for f,x in zip(\n",
    "        gtr_projects_df['funder_name'],\n",
    "           gtr_projects_df['discipline_sets'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modal_value(l):\n",
    "    c = Counter(l)\n",
    "    try:\n",
    "        return c.most_common(1)[0][0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "gtr_projects_df['modal_discipline'] = [modal_value(d) for d in gtr_projects_df['discipline_sets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['modal_discipline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(chain(*gtr_projects_df['discipline_sets'])).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = [True if len(s) > 0 else False for s in gtr_projects_df['discipline_sets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove projects without abstracts\n",
    "gtr_projects_df = gtr_projects_df[~pd.isnull(gtr_projects_df['abstract_texts'])]\n",
    "# remove projects with short abstracts\n",
    "gtr_projects_df = gtr_projects_df[gtr_projects_df['abstract_texts'].str.len() > 250]\n",
    "# remove projects with no labels\n",
    "n_labels = [True if len(s) > 0 else False for s in gtr_projects_df['discipline_sets']]\n",
    "gtr_projects_df = gtr_projects_df[n_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('ner')\n",
    "\n",
    "with open(os.path.join(raw_data_path, 'stopwords_en_long.txt'), 'r') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "for stopword in stopwords:\n",
    "    nlp.vocab[stopword.lower()].is_stop = True\n",
    "    nlp.vocab[stopword.upper()].is_stop = True\n",
    "    nlp.vocab[stopword.title()].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [remove_markup(a) for a in gtr_projects_df['abstract_texts']]\n",
    "abstracts = [normalise_digits(a) for a in abstracts]\n",
    "abstracts = lemmatize(abstracts, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrammer = Phraser.load(os.path.join(model_path, 'gtr_discipline_bigrammer.pkl'))\n",
    "abstracts = bigram(abstracts, phraser=bigrammer)\n",
    "abstracts_str = list(stringify_docs(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['abstract_processed'] = abstracts_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df['n_disciplines'] = [len(a) for a in gtr_projects_df['discipline_sets']]\n",
    "gtr_pure_df = gtr_projects_df[gtr_projects_df['n_disciplines'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_single = TfidfVectorizer(\n",
    "    max_df=0.4, \n",
    "    min_df=5,\n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "tfidf_pure_vecs = tfidf_single.fit_transform(gtr_pure_df['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_pure_vecs, gtr_pure_df['modal_discipline'], train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SelectPercentile(chi2, 40)\n",
    "sp_vecs = sp.fit_transform(X_train, y_train)\n",
    "sp_vecs_test = sp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=10, solver='lbfgs', multi_class='auto')\n",
    "mnb = MultinomialNB()\n",
    "cmb = ComplementNB()\n",
    "voter = VotingClassifier(estimators=[('lr', lr), ('cmb', cmb), ('mnb', mnb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100]}\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_params, cv=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid.fit(sp_vecs, y_train)\n",
    "lr_best = lr_grid.best_estimator_\n",
    "print(classification_report(y_test, lr_best.predict(sp_vecs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    pd.DataFrame(confusion_matrix(y_test, lr_best.predict(sp_vecs_test)), columns=mlb.classes_, index=mlb.classes_),\n",
    "    annot=True,\n",
    "    fmt='d'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_single = TfidfVectorizer(\n",
    "    max_df=0.4, \n",
    "    min_df=5,\n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "sp = SelectPercentile(chi2, 40)\n",
    "lr = LogisticRegression(C=10, solver='lbfgs', multi_class='auto')\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('tfidf', tfidf),\n",
    "        ('sp', sp),\n",
    "        ('lr', lr)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(gtr_pure_df['abstract_processed'], gtr_pure_df['modal_discipline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe, os.path.join(model_path, 'gtr_discipline_lvl9_lr_20190222.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.3, \n",
    "    min_df=5,\n",
    "    sublinear_tf=True, \n",
    "    norm='l2'\n",
    ")\n",
    "tfidf_vecs = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(chain(*gtr_projects_df['discipline_sets'])))\n",
    "mlb = MultiLabelBinarizer(classes=classes)\n",
    "target_binarized = mlb.fit_transform(gtr_projects_df['discipline_sets'])\n",
    "target_binarized_df = pd.DataFrame(target_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, target_binarized_df, train_size=0.8, test_size=0.2)\n",
    "sp = SelectPercentile(chi2, 40)\n",
    "sp_vecs = sp.fit_transform(X_train, y_train)\n",
    "sp_vecs_test = sp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', multi_class='auto')\n",
    "mnb = MultinomialNB()\n",
    "cmb = ComplementNB()\n",
    "voter = VotingClassifier(estimators=[('lr', lr), ('cmb', cmb), ('mnb', mnb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in mlb.classes_:\n",
    "    print(cls)\n",
    "    lr.fit(sp_vecs, y_train[cls])\n",
    "    print(classification_report(y_test[cls], lr.predict(sp_vecs_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, voter.predict(sp_vecs_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "     'bootstrap': [True, False],\n",
    "     'max_depth': [10, 20, 50, 100, None],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10],\n",
    "     'n_estimators': [100, 200, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=3)\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=rf_params,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf_random.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_estimator_.fit(tfidf_vecs_filt, target_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_random.best_estimator_, os.path.join(model_path, 'gtr_discipline_classifier_rf_8_20192102.pkl'))\n",
    "joblib.dump(tfidf, os.path.join(model_path, 'gtr_discipline_tfidf_20192102.pkl'))\n",
    "bigrammer.save(os.path.join(model_path, 'gtr_discipline_bigrammer.pkl'))\n",
    "nlp.vocab.to_disk(os.path.join(model_path, 'gtr_discipline_vocab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_projects_df = pd.read_csv(os.path.join(inter_data_path, 'fp7_h2020_projects.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_abstracts = [remove_markup(a) for a in cordis_projects_df['objective'][:25]]\n",
    "cordis_abstracts = [normalise_digits(a) for a in cordis_abstracts]\n",
    "cordis_abstracts = lemmatize(cordis_abstracts, nlp)\n",
    "cordis_abstracts = bigram(cordis_abstracts, phraser=bigrammer)\n",
    "cordis_abstracts = list(stringify_docs(cordis_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract, pred in zip(cordis_projects_df['objective'][:25], pipe.predict(cordis_abstracts)):\n",
    "    print(pred)\n",
    "    print(abstract)\n",
    "    print('\\n==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_tfidf_vecs = tfidf.transform(cordis_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_subject_probs = rf_random.best_estimator_.predict_proba(cordis_tfidf_vecs)\n",
    "cordis_subjects = rf_random.best_estimator_.predict(cordis_tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_probs = np.zeros((len(cordis_projects_df), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    subject_probs[:, i] = cordis_subject_probs[i][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cordis_projects_df['objective'][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cordis_subjects, columns=mlb.classes_).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = []\n",
    "indices = np.array(range(0, X_train.shape[1]))\n",
    "for discipline in y_train.columns:\n",
    "    features_chi2 = chi2(X_train, y_train[discipline])[0]\n",
    "    threshold = np.percentile(features_chi2[~pd.isnull(features_chi2)], 90)\n",
    "    discipline_indices = indices[features_chi2 > threshold]\n",
    "    feature_terms.extend(np.array(tfidf.get_feature_names())[discipline_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stop_words = set(tfidf.get_feature_names()).difference(set(feature_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "#     max_df=0.5, \n",
    "    min_df=5, \n",
    "    sublinear_tf=True, \n",
    "    norm='l2',\n",
    "    stop_words=tfidf_stop_words\n",
    ")\n",
    "tfidf_vecs_filt = tfidf.fit_transform(abstracts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs_filt, target_binarized, train_size=0.9, test_size=0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
