{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAIRE Data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import xmltodict\n",
    "import pyjq\n",
    "import boto3\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from eu_funding.visualization.visualize import pdf_cdf\n",
    "from eu_funding.utils.misc_utils import print_nested_structure\n",
    "from eu_funding.data.s3_transfer import get_files_from_s3\n",
    "from eu_funding.data.openaire import parse_openaire_records, parse_publications_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'im-eurito'\n",
    "FOLDER = 'external/openaire/projectssoups'\n",
    "KEY_PREFIX = 'soup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(get_files_from_s3(bucket=BUCKET, folder=FOLDER, key_prefix=KEY_PREFIX)):\n",
    "    if i > 0:\n",
    "        break\n",
    "    soup = BeautifulSoup(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for file in get_files_from_s3(bucket=BUCKET, folder=FOLDER, key_prefix=KEY_PREFIX):\n",
    "    records.extend(parse_openaire_records(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(inter_data_path, 'openaire_projects.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'im-eurito'\n",
    "FOLDER = 'external/openaire/publicationssoups'\n",
    "KEY_PREFIX = 'soup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_publications():\n",
    "    records = []\n",
    "    for file in os.listdir(os.path.join(openaire_publication_data_path)):\n",
    "        file_number = file.split('.')[0].split('_')[-1]\n",
    "        if '.txt' in file:\n",
    "            with open(os.path.join(openaire_publication_data_path, file), mode='rb') as f:\n",
    "                data = f.read()\n",
    "                soup = BeautifulSoup(data)\n",
    "                rec = parse_publications_soup(soup)\n",
    "                records.extend(rec)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_publications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i, record in enumerate(chunks(records, 1000)):\n",
    "    i += 1\n",
    "    df = pd.DataFrame().from_records(record)\n",
    "    df.to_csv(\n",
    "        os.path.join(openaire_publication_data_path, 'csv', 'publications_parsed_{:03}.csv'.format(i)),\n",
    "        index=False\n",
    "    )\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publications_df = pd.concat(dfs)\n",
    "publications_df = pd.read_csv(os.path.join(inter_data_path, 'openaire_publications_20190702.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Missing PubMed DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from eu_funding.utils.misc_utils import chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_converter(pub_ids, id_type):\n",
    "    id_converter_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/'\n",
    "    params = {\n",
    "        'idtype': id_type,\n",
    "        'ids': ', '.join([str(i) for i in pub_ids]),\n",
    "        'email': 'george.richardson@nesta.org.uk',\n",
    "        'tool': 'eu_funding_analytics'\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url=id_converter_url,\n",
    "        params=params,\n",
    "    )\n",
    "    results = response.content\n",
    "    return results\n",
    "    \n",
    "def parse_id_converter_result(results, id_type):\n",
    "    soup = BeautifulSoup(results)\n",
    "    records = [record.attrs for record in soup.findAll('record')]\n",
    "    for r in records:\n",
    "        r['pid_type'] = id_type\n",
    "    return records\n",
    "\n",
    "def convert_ids(pub_ids, id_type):\n",
    "    pub_id_chunks = chunks(pub_ids, 200)\n",
    "    converted = []\n",
    "    for chunk in pub_id_chunks:\n",
    "        results = get_id_converter(chunk, id_type)\n",
    "        records = parse_id_converter_result(results, id_type)\n",
    "        converted.extend(records)\n",
    "        sleep(1)\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_type = 'pmid'\n",
    "pub_ids = publications_df[publications_df['pid_type'] == id_type]['pid'].values\n",
    "\n",
    "pmid_converted_ids = convert_ids(pub_ids, id_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doi_col(pid, pid_type):\n",
    "    if pid_type == 'doi':\n",
    "        return pid\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "publications_df['doi'] = publications_df['pid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_map(element, mapping):\n",
    "    if element in mapping:\n",
    "        return mapping[element]\n",
    "    else:\n",
    "        return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_df = pd.DataFrame().from_records(pmid_converted_ids)\n",
    "pmid_doi_map = {k: v for k, v in zip(\n",
    "    pmid_df['pmid'], pmid_df['doi']\n",
    ")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df['doi'] = publications_df['doi'].apply(lambda x: apply_map(x, pmid_doi_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_type = 'pmc'\n",
    "pub_ids = publications_df[publications_df['pid_type'] == id_type]['pid'].values\n",
    "\n",
    "pmcid_converted_ids = convert_ids(pub_ids, 'pmcid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid_df = pd.DataFrame().from_records(pmcid_converted_ids)\n",
    "pmcid_doi_map = {k.lower(): v for k, v in zip(\n",
    "    pmcid_df['pmcid'], pmcid_df['doi']\n",
    ")}\n",
    "publications_df['doi'] = publications_df['doi'].apply(lambda x: apply_map(x, pmcid_doi_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.to_csv(os.path.join(inter_data_path, 'openaire_publications.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(publications_df[\n",
    "    (publications_df['pid'] != 'pmid') \n",
    "    & (publications_df['pid'] != 'pmc')\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAK Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabet_detector import AlphabetDetector\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text as sql_text\n",
    "\n",
    "# Inputs for the MAK POST request, including the API key\n",
    "HEADERS = {\n",
    "    'Ocp-Apim-Subscription-Key': '4774550073674321a53be3e28595c92c',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "# Fields to return from MAK\n",
    "FIELDS = [\"Id\",\"Ti\",\"D\",\"AA.AuN\",\"AA.AuId\",\"F.FId\",\"L\",\"C.CN\",\"E\",\n",
    "          \"J.JId\",\"AA.AfId\",\"CC\",\"ECC\",\"AA.AfN\",\"J.JN\",\"F.FN\"]\n",
    "\n",
    "\n",
    "class TitleProcessor(AlphabetDetector):\n",
    "    '''Processes a pure utf-8 title into something ready for a MAK query.'''\n",
    "    def process_title(self, title):\n",
    "        # Get replace non-alphanums (allowing foreign characters)\n",
    "        result = \"\".join([x\n",
    "                          if len(self.detect_alphabet(x)) > 0\n",
    "                          or x.isnumeric()\n",
    "                          else \" \" for x in title.lower()])\n",
    "        # Replace double-spaces with single-spaces\n",
    "        while \"  \" in result:\n",
    "            result = result.replace(\"  \",\" \")\n",
    "        return result.strip()\n",
    "\n",
    "\n",
    "'''Find matches to titles from the MAK database.\n",
    "\n",
    "    raw_titles: A list of titles in the form (id, title)\n",
    "    call_limit: The maximum number of MAK API calls. \n",
    "                NB: Nesta's allowance is 10,000 per month.\n",
    "'''\n",
    "def mak_from_titles(raw_titles, call_limit, optional_columns, title_offset=0):\n",
    "\n",
    "    # Make arXiv titles match MAK title format (strip non-alphanums,\n",
    "    # allowing foreign chars)\n",
    "    tp = TitleProcessor()\n",
    "    titles = [(pid,tp.process_title(t)) for pid,t in raw_titles]\n",
    "    # Maximum of title_count titles, returning query_count results\n",
    "    title_count = 500\n",
    "#     title_offset = 0\n",
    "    query_count = 1000\n",
    "    char_limit = 16000\n",
    "\n",
    "    # Count the number of calls for book-keeping\n",
    "    calls = 0\n",
    "\n",
    "    # Iterate until done\n",
    "    data = []\n",
    "    while title_offset < len(titles):\n",
    "        records = []\n",
    "        # A soft limit so that we don't overrun the API limit\n",
    "        if calls >= call_limit:\n",
    "            break\n",
    "        calls += 1\n",
    "        \n",
    "        first_title = title_offset\n",
    "        print('Querying from {}'.format(first_title))\n",
    "        # Generate the MAK query (OR statement of titles (Ti))\n",
    "        expr_titles = \"\"\n",
    "\n",
    "        while (len(expr_titles) < char_limit) & (title_offset < len(titles)):\n",
    "            expr_titles = expr_titles + \"Ti='{}',\".format(titles[title_offset][1])\n",
    "            title_offset += 1\n",
    "        \n",
    "        titles_subset = titles[first_title:title_offset]\n",
    "        expr = [\"Ti='\"+t+\"'\" for _,t in titles_subset]\n",
    "        print(\"Posting\",len(expr),\"queries\")\n",
    "        expr = ','.join(expr)\n",
    "        expr = \"expr=OR(\"+expr+\")\"\n",
    "#         print(expr)\n",
    "        \n",
    "        # Write and launch the query\n",
    "        query = expr+\"&count=\"+str(query_count)+\"&attributes=\"+\",\".join(FIELDS)\n",
    "        r = requests.post('https://api.labs.cognitive.microsoft.com/academic/v1.0/evaluate',\n",
    "                          data=query.encode(\"utf-8\"), headers=HEADERS)\n",
    "        try:\n",
    "            js = r.json()\n",
    "        except json.decoder.JSONDecodeError as err:\n",
    "            print(\"Error with status code \",r.status_code)\n",
    "            print(r.text)\n",
    "            raise err\n",
    "        try:\n",
    "            print(\"Got\",len(js[\"entities\"]),\"results\")\n",
    "        except KeyError as err:\n",
    "            print(r.status_code)\n",
    "            print(r.text)\n",
    "        \n",
    "        # Append the results to the output\n",
    "        for pid, t in titles_subset:\n",
    "            # Flag in case no match is found\n",
    "            matched = False\n",
    "            for row in js[\"entities\"]:\n",
    "                if t != row[\"Ti\"]:\n",
    "                    continue\n",
    "                matched = True\n",
    "                break\n",
    "            # Default in case no match is found\n",
    "            if not matched:\n",
    "                data.append(dict(pid=pid,title=t,matched=False))\n",
    "                continue\n",
    "            # If a match was found, extract info        \n",
    "            insts = list(set(author[\"AfN\"] for author in row[\"AA\"] if \"AfN\" in author))\n",
    "\n",
    "            # Convert \"extended metadata\" (E) to json, then extract arxiv IDs\n",
    "            arxiv_sources = []\n",
    "            if \"E\" in row:\n",
    "                if type(row[\"E\"]) is not dict:\n",
    "                    row[\"E\"] = json.loads(row[\"E\"])\n",
    "                if 'S' in row[\"E\"]:\n",
    "                    for source in row[\"E\"][\"S\"]:\n",
    "                        if \"U\" not in source:\n",
    "                            continue\n",
    "                        if not source['U'].startswith(\"https://arxiv.org/\"):\n",
    "                            continue\n",
    "                        arxiv_sources.append(source['U'])\n",
    "            if \"F\" in row:\n",
    "                field_names = []\n",
    "                for f in row[\"F\"]:\n",
    "                    field_names.append(f['FN'])\n",
    "            # Add then mandatory fields\n",
    "            data_row = dict(pid=pid,title=t, institutes=insts, arxiv_sources=arxiv_sources,\n",
    "                            citations=row[\"CC\"], date=row[\"D\"], field_names=field_names, matched=True)            \n",
    "            # Then add optional fields\n",
    "            for long, short in optional_columns.items():                \n",
    "                second = None\n",
    "                if \".\" in short:\n",
    "                    short, second = short.split(\".\")\n",
    "                if short in row:\n",
    "                    if second is None:\n",
    "                        data_row[long] = row[short]\n",
    "                    elif second in row[short]:\n",
    "                        data_row[long] = row[short][second]\n",
    "            records.append(data_row)\n",
    "            with open(os.path.join(\n",
    "                ext_data_path,\n",
    "                'mak', \n",
    "                'openaire_publications', \n",
    "                f'mak_oa_publications_{first_title}_{title_offset}.json'), 'w') as f:\n",
    "                json.dump(records, f)\n",
    "        data.extend(records)\n",
    "    # Print summary statistics\n",
    "    nmatch = 0 \n",
    "    nboth = 0\n",
    "    for row in data:\n",
    "        if not row[\"matched\"]:\n",
    "            continue\n",
    "        nmatch += 1\n",
    "        if row[\"citations\"] > 0 and len(row[\"institutes\"]) > 0:\n",
    "            nboth += 1\n",
    "    print(\"Made\",calls,\"calls\")\n",
    "    print(\"Got\",nmatch,\"matches from\",len(data),\"queries, of which\",\n",
    "          nboth,\"contained both institutes and citation information\")\n",
    "    # Done\n",
    "    return data\n",
    "\n",
    "# Stolen from https://stackoverflow.com/a/434328/1571593\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "# Execute IN statements in chunks\n",
    "def execute_IN_in_chunks(con, query, chunkable, chunk_size):\n",
    "    output = []\n",
    "    for chunk in chunker(chunkable, chunk_size):\n",
    "        result = con.execute(sql_text(query), values=tuple(chunk))\n",
    "        output += result.fetchall()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df['title'].fillna('Title Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2716\n",
    "optional_columns = dict(language=\"L\", full_title=\"E.DN\",\n",
    "                        conference=\"CN\", journal=\"E.BV\", doi=\"E.DOI\")\n",
    "\n",
    "data = mak_from_titles(\n",
    "    zip(publications_df['index'], publications_df['title'].values), call_limit=500,\n",
    "    optional_columns=optional_columns, title_offset=n\n",
    ")\n",
    "df_magapi = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"expr=OR(Ti='from housing as asset to housing as patrimony policy ideas and the re emergence of the housing question',Ti='experimental analysis of nonlinear impairments in fibre optic transmission systems up to 7 3 thz',Ti='a novel method to measure electronic spectra of cold molecular ions',Ti='dichotomy of short and long thymic stromal lymphopoietin isoforms in inflammatory disorders of the bowel and skin',Ti='tumor associated macrophages as treatment targets in oncology',Ti='quesp and quest revisited fast and accurate quantitative cest experiments',Ti='system level analysis of swipt mimo cellular networks',Ti='antagonist properties of monoclonal antibodies targeting human cd28 role of valency and the heavy chain constant domain',Ti='alemannen franken pfalz oberrhein von den versuchen der landesgeschichte eine heimat zu geben',Ti='l2 induced gain for discrete time switched lur e systems via a suitable lyapunov function',Ti='production of scopularide a in submerged culture with scopulariopsis brevicaulis',Ti='youth political participation in a transition society',Ti='compile time function memoization',Ti='liveness through the lens of agency and causality',Ti='targeting the tumor and its microenvironment by a dual function decoy met receptor',Ti='observation of ph induced protein reorientation at the water surface',Ti='improving uncoordinated collaboration in partially observable domains with imperfect simultaneous action communication',Ti='computational invention of cadences and chord progressions by conceptual chord blending',Ti='energy efficient design for edge caching wireless networks when is coded caching beneficial ',Ti='role of dc sign in lassa virus entry into human dendritic cells ',Ti='mesoscopic moment equations for heat conduction characteristic features and slow fast mode decomposition',Ti='marker free phenotyping of tumor cells by fractal analysis of reflection interference contrast microscopy images',Ti='autonomous reinforcement of behavioral sequences in neural dynamics',Ti='concurrent number cruncher a gpu implementation of a general sparse linear solver',Ti='challenges for nanomechanical sensors in biological detection',Ti='measuring economic complexity of countries and products which metric to use ',Ti='youth prospects in a time of economic recession',Ti='the continuing value of twin studies in the omics era',Ti='magnetization and anisotropy of cobalt ferrite thin films',Ti='analysis of pectin mutants and natural accessions of arabidopsis highlights the impact of de methyl esterified homogalacturonan on tissue saccharification',Ti='general automatic human shape and motion capture using volumetric contour cues',Ti='clinical research on neglected tropical diseases challenges and solutions',Ti='individual nsaids and upper gastrointestinal complications',Ti='combinatorial flexibility of cytokine function during human t helper cell differentiation',Ti='analysis of the cross talk of epstein barr virus infected b cells with t cells in the marmoset',Ti='selecting translations to be post edited by sentence level automatic quality evaluation',Ti='pyrochar decomposition under french grassland monitored by 13c natural abundance',Ti='abiotic and biotic processes governing the fate of phenylurea herbicides in soils a review',Ti='dynamic herd simulations cownex a tool to assess nitrogen excretion and efficiency of a dairy cattle herd according management',Ti='the relevance of light in the formation of colloidal metal nanoparticles',Ti='how anatomy shapes dynamics a semi analytical study of the brain at rest by a simple spin model',Ti='darboux integrability and algebraic limit cycles for a class of polynomial differential systems',Ti='atm splicing variants as biomarkers for low dose dexamethasone treatment of a t',Ti='anisotropic critical state theory role of fabric',Ti='beliefs about others intentions determine whether cooperation is the faster choice',Ti='mycotoxin biotransformation by native and commercial enzymes present and future perspectives',Ti='ge mediated surface preparation for twin free 3c sic nucleation and growth on low off axis 4h sic substrate',Ti='participatory modelling to support decision making in water management under uncertainty two comparative case studies in the guadiana river basin spain',Ti='innovative instrumentation for eurisol report on the fifth eurisol user group topical meeting the ron cooke hub heslington east campus univ of york uk 15 17 july 2014',Ti='somos o que comemos',Ti='corporate governance value and performance of firms new empirical results on convergence from a large international database',Ti='sharing data for public security',Ti='fgf21 and cardiac physiopathology',Ti='pace simple multi hop scheduling for single radio 802 11 based stub wireless mesh networks',Ti='a simulation study of local defect resonances ldr ',Ti='openaire guidelines 1 1 guidelines for content providers of the openaire information space',Ti='physiologically based pharmacokinetic modeling of perfluoroalkyl substances in the human body',Ti='ethylene carbonate free adiponitrile based electrolytes compatible with graphite anodes',Ti='let me guide you pedagogical interaction style for a robot in children s education',Ti='trinocchio privacy preserving outsourcing by distributed verifiable computation',Ti='mapping phytoplankton blooms in deep subalpine lakes from sentinel 2a and landsat 8',Ti='requirements document wp3 d3 1',Ti='mirri policy on accession',Ti='photonics4all start up challenge report',Ti='injectable rectifiers as microdevices for remote electrical stimulation an alternative to inductive coupling',Ti='systems medicine and integrated care to combat chronic noncommunicable diseases',Ti='correlations between islet autoantibody specificity and the slc30a8 genotype with hla dqb1 and metabolic control in new onset type 1 diabetes',Ti='environmental regulation and competitiveness empirical evidence on the porter hypothesis from european manufacturing sectors',Ti='clinical pet imaging of insulinoma and beta cell hyperplasia',Ti='inhibiting receptor tyrosine kinase axl with small molecule inhibitor bms 777607 reduces glioblastoma growth migration and invasion in vitro and in vivo',Ti='democritus an adaptive particle in cell pic code for object plasma interactions',Ti=' mygreenservices un projet en mode living lab pilote par inria sophia antipolis relatif a la co creation de services environnementaux bases sur des capteurs citoyens ville de nice ',Ti='le role du lobe temporal median dans les liens entre musiques et paroles une approche en neuropsychologie et neuro imagerie',Ti='traitrecordj a programming language with traits and records',Ti='observation of poiseuille flow of phonons in black phosphorus',Ti='guanylate binding protein 5 impairing virion infectivity by targeting retroviral envelope glycoproteins',Ti='intercontinental karyotype environment parallelism supports a role for a chromosomal inversion in local adaptation in a seaweed fly',Ti='multiprocessor scheduling of precedence constrained mixed critical jobs',Ti='effect of the surface structure of pt 100 and pt 110 on the oxidation of carbon monoxide in alkaline solution an ftir and electrochemical study',Ti='microbial inhibition of oral epithelial wound recovery potential role for quorum sensing molecules ',Ti='psychological complaints among children in joint physical custody and other family types considering parental factors',Ti='defect induced local variation of crystal phase transition temperature in metal halide perovskites',Ti='electra irp voltage control strategy for enhancing power system stability in future grid architectures',Ti='the phenotypic architecture of tetraploid wheat triticum turgidum l effects of domestication and post domestication under contrasting nitrogen fertilisation',Ti='multi area network model of visual cortex',Ti='gearing motion in cogwheel pairs of molecular rotors weak coupling limit',Ti='pectenotoxin s abcde ring system a complex target to test the potential of singlet oxygen super cascades as tools for synthesis',Ti='growth of krskopolje piglets during lactation and first rearing period',Ti='evilinhd a virtual research environment open and collaborative for dh scholars',Ti='optimisation of code saturne for petascale simulations',Ti='avalokitesvara of the six syllables locating the practice of the great vehicle in the landscape of central india',Ti='release of a live elixir communication strategy',Ti='electricity in hpc centres',Ti='modeling simulation and comparison of control techniques for energy storage systems',Ti='d3 1 evaluation of systematic relations between the seismic response to fluid injection and depth injection pressure crustal stress state and local structural geology',Ti='flexible multi layer sparse approximations of matrices and applications',Ti='partitioning of trace elements and metals between quasi ultrafine accumulation and coarse aerosols in indoor and outdoor air in schools',Ti='enhancing location related hydrogeological knowledge',Ti='bistability breaks off deterministic responses to intracortical stimulation during non rem sleep',Ti='a gamma moment approach to monotonic boundary estimation',Ti='serial defaults serial profits returns to sovereign lending in habsburg spain 1566 1600',Ti='crystal structure and proton conductivity of basn0 6sc0 4o3 d insights from neutron powder diffraction and solid state nmr spectroscopy electronic supplementary information esi available rietveld fit of dry basn0 6sc0 4o3 d sample fig s1 119sn fig s2 45sc fig s3 s6 and 17o fig s7 spectra of all materials as a function of sc doping concentration 45sc mqmas of deuterated basn0 9sc0 1o3 d fig s4 45sc mqmas of dry and deuterated basn0 8sc0 2o3 d fig s5 45sc mqmas of dry and deuterated basn0 7sc0 3o3 d fig s6 17o mqmas of 17o enriched basn0 8sc0 2o3 d and basn0 6sc0 4o3 d fig s8 see doi 10 1039 c5ta09744d click here for additional data file ',Ti='genome sequence of bluetongue virus type 2 from india evidence for reassortment between outer capsid protein genes',Ti='accurate nuclear radii and binding energies from a chiral interaction',Ti='determining projection constants of univariate polynomial spaces',Ti='co transcriptional histone h2b monoubiquitylation is tightly coupled with rna polymerase ii elongation rate',Ti='impaired high density lipoprotein anti oxidant capacity in human abdominal aortic aneurysm',Ti='theoretical vibrational excitation cross sections and rate coefficients for electron impact resonant collisions involving rovibrationally excited n2 and no molecules',Ti='predicting species maximum dispersal distances from simple plant traits',Ti='mapping the surface adsorption forces of nanomaterials in biological systems',Ti='a novel approach for arsenic adsorbents regeneration using mgo',Ti='climate events synchronize the dynamics of a resident vertebrate community in the high arctic',Ti='efficient engineering of a bacteriophage genome using the type i e crispr cas system',Ti='effects of acceleration on gait measures in three horse gaits',Ti='internalization assays for listeria monocytogenes ',Ti='drivers phone use at red traffic lights a roadside observation study comparing calls and visual manual interactions',Ti='deformations of gr and bh thermodynamics',Ti='do kenya s climate change mitigation ambitions necessitate large scale renewable energy deployment and dedicated low carbon energy policy ',Ti='ultra high field mri post mortem structural connectivity of the human subthalamic nucleus substantia nigra and globus pallidus',Ti='cultural property',Ti='a new method for focal transient cerebral ischaemia by distal compression of the middle cerebral artery',Ti='patient safety in primary care a survey of general practitioners in the netherlands',Ti='decadal prediction skill in a multi model ensemble',Ti='the distinct role of the amygdala superior colliculus and pulvinar in processing of central and peripheral snakes',Ti='collision of almost parallel vortex filaments',Ti='atomic model of a cell wall cross linking enzyme in complex with an intact bacterial peptidoglycan',Ti='hierarchical reinforcement learning and central pattern generators for modeling the development of rhythmic manipulation skills',Ti='prospects for laser spectroscopy of highly charged ions with high harmonic xuv and soft x ray sources',Ti='evolution of theories of mind',Ti='tsi metamodels based multi objective robust optimization',Ti='de novo active sites for resurrected precambrian enzymes',Ti='facial colorings using hall s theorem',Ti='on sat technologies for dependency management and beyond',Ti='protein co evolution how do we combine bioinformatics and experimental approaches ',Ti='large spin relaxation anisotropy and valley zeeman spin orbit coupling in wse2 graphene h bn heterostructures',Ti='fasciola and fasciolosis in ruminants in europe identifying research needs',Ti='stochastic models of population extinction',Ti='high quality polarization entanglement state preparation and manipulation in standard telecommunication channels',Ti='the alma protostellar interferometric line survey pils first results from an unbiased submillimeter wavelength line survey of the class 0 protostellar binary iras 16293 2422 with alma',Ti='combined deterministic and stochastic approaches for modelling the evolution of food products along the cold chain part ii a case study',Ti='optical signal to noise ratio improvement through unbalanced noise beating in phase sensitive parametric amplifiers',Ti='interpreting multiple dualities conjectured from superconformal index identities',Ti='an efficient method to assemble linear dna templates for in vitro screening and selection systems',Ti='trust anchors in software defined networks',Ti='identification of trypanosoma cruzi discrete typing units dtus through the implementation of a high resolution melting hrm genotyping assay',Ti='artificially lit surface of earth at night increasing in radiance and extent',Ti='circularly polarized modes in magnetized spin plasmas',Ti='chronic obstructive pulmonary disease patient journey hospitalizations as window of opportunity for extra pulmonary intervention',Ti='superhydrophobic paper from nanostructured fluorinated cellulose esters',Ti='potential of natural biocides for biocontrolling phototrophic colonization on limestone',Ti='emerging techniques and exotic systems frontiers of photoionization photodetachment',Ti='single pion energy resolution of a high granularity scintillator calorimeter system',Ti='zebrafish as a model for kidney function and disease',Ti='topological order and thermal equilibrium in polariton condensates',Ti='holocene north atlantic overturning in an atmosphere ocean sea ice model compared to proxy based reconstructions',Ti='magnetically driven anisotropic structural changes in the atomic laminate mn2gac',Ti='life without geminin',Ti='monoclonal igg antibodies generated from joint derived b cells of ra patients have a strong bias toward citrullinated autoantigen recognition',Ti='population structure of atlantic mackerel scomber scombrus ',Ti='run time interoperability between neuronal network simulators based on the music framework',Ti='relationship between environmental factors dry matter loss and mycotoxin levels in stored wheat and maize infected withfusariumspecies',Ti='a fiscal union for the emu ',Ti='postoperative pain management in spanish hospitals a cohort study using the pain out registry',Ti='simulation study of cochlear implants stimulation protocols and its application to surgical planning',Ti='jornadas 2010 do departamento de quimica',Ti='why we need a token based typology a case study of analytic and lexical causatives in fifteen european languages',Ti='the grand challenge of characterizing ribonucleoprotein networks',Ti='phenomenological fingerprints of four meditations differential state changes in affect mind wandering meta cognition and interoception before and after daily practice across 9 months of training')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TitleProcessor()\n",
    "x = [unidecode(t.process_title(s)) for s in publications_df['title'][2716:2716+180]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eu_funding.utils.nlp_utils import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doi_crossref(title, max_rows=5):\n",
    "    title = title.lower()\n",
    "    r = requests.get(\n",
    "    'https://api.crossref.org/works?rows=5&query.title={}'.format(title)\n",
    "    )\n",
    "    doi = np.nan\n",
    "    if r.status_code == 200:\n",
    "        j = r.json()\n",
    "        results = j['message']['items']\n",
    "        dist_max = 0\n",
    "\n",
    "        for result in results:\n",
    "            result_title = result['title'][0].lower()\n",
    "            dist = fuzz.ratio(title, result_title)\n",
    "            if dist < 90:\n",
    "                continue\n",
    "            elif dist == 100:\n",
    "                doi = result['DOI']\n",
    "            elif 100 > dist >= 90:\n",
    "                if dist > dist_max:\n",
    "                    doi = result['DOI']\n",
    "                    dist_max = dist\n",
    "    return doi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Etiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eu_funding.utils.misc_utils import chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = publications_df['title'][pd.isnull(publications_df['doi'])].str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = 20\n",
    "timeout = 30\n",
    "\n",
    "for i, titles in enumerate(chunks(all_titles, 1000)):\n",
    "    out = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=connections) as executor:\n",
    "        future_to_url = (executor.submit(get_doi_crossref, title.decode(), timeout) for title in titles)\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            try:\n",
    "                data = future.result()\n",
    "            except Exception as exc:\n",
    "                data = str(type(exc))\n",
    "            finally:\n",
    "                out.append(data)\n",
    "                \n",
    "    with open(os.path.join(inter_data_path, 'openaire_missing_dois', 'dois_{:03}.txt'.format(i)), 'w') as f:\n",
    "        for o in out:\n",
    "            f.write(str(o) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dois = []\n",
    "files = os.listdir(os.path.join(inter_data_path, 'openaire_missing_dois'))\n",
    "for file in files:\n",
    "    with open(os.path.join(inter_data_path, 'openaire_missing_dois', file), 'r') as f:\n",
    "        missing_dois.extend(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df['doi'].loc[all_titles.index] = missing_dois\n",
    "publications_df['doi'][publications_df['doi'] == 'nan'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df.to_csv(os.path.join(inter_data_path, 'openaire_publications_20190702.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossRef Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Etiquette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquette = Etiquette(\n",
    "    application_version='0.1',\n",
    "    application_url='http://www.eurito.eu/',\n",
    "    application_name='eu_funding_analytics',\n",
    "    contact_email='george.richardson@nesta.org.uk',   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crossref_work(doi):\n",
    "    works = Works(etiquette=etiquette)\n",
    "    response = works.doi(doi)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dois = publications_df['doi'][~pd.isnull(publications_df['doi'])].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dois = all_dois[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_chunks = list(chunks(all_dois, 1000))\n",
    "doi_chunk_indices = list(range(len(doi_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "connections = 2 # API will rate limit occasionally with just 2 connections so needs babysitting\n",
    "\n",
    "for i, dois in zip(doi_chunk_indices[start:], doi_chunks[start:]):\n",
    "    out = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=connections) as executor:\n",
    "        future_to_url = (executor.submit(get_crossref_work, doi) for doi in dois)\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            data = future.result()\n",
    "            out.append(data)\n",
    "                \n",
    "    with open(os.path.join(ext_data_path, 'crossref', 'works_{:04}.txt'.format(i)), 'w') as f:\n",
    "        json.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
